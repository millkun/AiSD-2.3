from flask import Flask, request, jsonify
import numpy as np
import os
import time
import torch
import torch.nn as nn
from torchvision.models import resnet18
import librosa
import librosa.display
import matplotlib
matplotlib.use('Agg')  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º non-interactive backend –î–û –∏–º–ø–æ—Ä—Ç–∞ pyplot
import matplotlib.pyplot as plt
import pickle

app = Flask(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –ú–ö!)
SAMPLE_RATE = 16000
AUDIO_FOLDER = "raw_audio"
SPECTROGRAM_FOLDER = "spectrograms"
BUFFER_SIZE = 24000  # 24000 —Å—ç–º–ø–ª–æ–≤ (1.5 —Å–µ–∫)
CHUNK_SIZE = 2048    # –ö–∞–∫ –≤ –ø—Ä–æ—à–∏–≤–∫–µ –ú–ö
EXPECTED_CHUNKS = (BUFFER_SIZE + CHUNK_SIZE - 1) // CHUNK_SIZE # 12 —á–∞–Ω–∫–æ–≤ (24000/2048)

os.makedirs(AUDIO_FOLDER, exist_ok=True)
os.makedirs(SPECTROGRAM_FOLDER, exist_ok=True)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ (–¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ–±—É—á–∞—é—â–∏–º –∫–æ–¥–æ–º)
class ResNetSpectrogram(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.resnet = resnet18(pretrained=False)
        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=0, bias=False)
        self.resnet.avgpool = nn.Identity()
        self.resnet.fc = nn.Identity()
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def forward(self, x):
        x = self.resnet.conv1(x)
        x = self.resnet.bn1(x)
        x = self.resnet.relu(x)
        x = self.resnet.maxpool(x)
        
        x = self.resnet.layer1(x)
        x = self.resnet.layer2(x)
        x = self.resnet.layer3(x)
        x = self.resnet.layer4(x)
        
        x = self.adaptive_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
def load_model(model_path, num_classes=3):
    model = ResNetSpectrogram(num_classes=num_classes)
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.eval()
    return model

# –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã
def create_mel_spec(audio, sr=16000, n_mels=64, n_fft=512, hop_length=512):
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ
    audio = audio.astype(np.float32)
    audio = audio / np.max(np.abs(audio) + 1e-10)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–º–ø–ª–∏—Ç—É–¥
    
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec_db

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã
def normalize(spec):
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –Ω—É–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
    if np.max(spec) - np.min(spec) == 0:
        return np.zeros_like(spec)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1] —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    norm_spec = (spec - np.min(spec)) / (np.max(spec) - np.min(spec) + 1e-10)
    return norm_spec

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
def prepare_input(audio):
    # –î–æ–±–∞–≤–ª—è–µ–º 600 –∫ —Å—ã—Ä–æ–º—É —Å–∏–≥–Ω–∞–ª—É (–ø–µ—Ä–µ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º –≤ float)
    audio = audio.astype(np.float32)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-1, 1] (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    # audio = audio / 32768.0  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ librosa —Ç—Ä–µ–±—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É
    mel_spec = create_mel_spec(audio)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É
    input_data = normalize(mel_spec)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (1, 1, H, W)
    input_tensor = torch.FloatTensor(input_data).unsqueeze(0).unsqueeze(0)
    
    return input_tensor, mel_spec

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def save_spectrogram_image(spec, filename):
    try:
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(spec, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel-frequency spectrogram')
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()  # –Ø–≤–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ —Ñ–∏–≥—É—Ä—ã
    except Exception as e:
        print(f"Error saving spectrogram: {e}")

# –ö–ª–∞—Å—Å—ã –∫–æ–º–∞–Ω–¥ (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ–±—É—á–µ–Ω–∏–µ–º)
CLASS_NAMES = ["–≤—Ä–µ–º—è", "–¥–∞—Ç–∞", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "—à—É–º"]  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = load_model("best_model.pth", num_classes=len(CLASS_NAMES))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
current_session = {
    "id": None,
    "chunks": [],
    "total_samples": 0
}

@app.route('/process_audio', methods=['POST'])
def process_audio():
    global current_session

    try:
        data = request.get_json()
        
        if 'audio' not in data:
            return jsonify({"error": "No audio data"}), 400

        chunk_index = data["chunk_index"]
        total_chunks = data["total_chunks"]
        audio_chunk = np.array(data["audio"], dtype=np.int16)
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ ‚Äî –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
        if chunk_index == 0:
            current_session["id"] = int(time.time())
            current_session["chunks"] = []
            current_session["total_samples"] = 0
            print(f"\nüî• –ù–∞—á–∞—Ç–∞ –Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è {current_session['id']}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —á–∞–Ω–∫ –ø—Ä–∏—à–µ–ª –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏
        if len(current_session["chunks"]) != chunk_index:
            print(f"‚õî –û—à–∏–±–∫–∞ –ø–æ—Ä—è–¥–∫–∞ —á–∞–Ω–∫–æ–≤! –û–∂–∏–¥–∞–ª—Å—è {len(current_session['chunks'])}, –ø–æ–ª—É—á–µ–Ω {chunk_index}")
            return jsonify({"error": "Invalid chunk order"}), 400

        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä
        current_session["chunks"].append(audio_chunk)
        current_session["total_samples"] += len(audio_chunk)
        
        print(f"‚úÖ –ß–∞–Ω–∫ {chunk_index + 1}/{total_chunks} –ø—Ä–∏–Ω—è—Ç ({len(audio_chunk)} —Å—ç–º–ø–ª–æ–≤)")

        # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        if chunk_index == total_chunks - 1:
            if current_session["total_samples"] >= BUFFER_SIZE - CHUNK_SIZE:  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                if current_session["total_samples"] != BUFFER_SIZE:
                    print(f"‚õî –û—à–∏–±–∫–∞! –ü–æ–ª—É—á–µ–Ω–æ {current_session['total_samples']}, –æ–∂–∏–¥–∞–ª–æ—Å—å {BUFFER_SIZE}")
                    return jsonify({"error": "Incomplete audio data"}), 400

            # –°–∫–ª–µ–∏–≤–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
            full_audio = np.concatenate(current_session["chunks"])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
            audio_filename = f"{AUDIO_FOLDER}/audio_{current_session['id']}.pkl"
            with open(audio_filename, 'wb') as f:
                pickle.dump(full_audio, f)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
            input_tensor, mel_spec = prepare_input(full_audio)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            spec_image_filename = f"{SPECTROGRAM_FOLDER}/spec_{current_session['id']}.png"
            save_spectrogram_image(mel_spec, spec_image_filename)
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = model(input_tensor)
                _, predicted = torch.max(outputs, 1)
                command = CLASS_NAMES[predicted.item()]
            
            print(f"\nüíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {audio_filename}")
            print(f"üìä –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {spec_image_filename}")
            print(f"ü§ñ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")
            
            return jsonify({
                "status": "success",
                "samples_received": full_audio.size,
                "command": command
            })
        else:
            return jsonify({"status": "chunk_received"})

    except Exception as e:
        print(f"‚õî –û—à–∏–±–∫–∞: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    print(f"üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ—Ç—Å—è {EXPECTED_CHUNKS} —á–∞–Ω–∫–æ–≤ –ø–æ {CHUNK_SIZE} —Å—ç–º–ø–ª–æ–≤")
    print(f"üîä –ö–ª–∞—Å—Å—ã –∫–æ–º–∞–Ω–¥: {CLASS_NAMES}")
    app.run(host='0.0.0.0', port=5000, debug=True)
