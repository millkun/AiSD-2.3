from flask import Flask, request, jsonify
import numpy as np
import os
import time
import torch
import torch.nn as nn
from torchvision.models import resnet18
import librosa
import librosa.display
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pickle


from fill_data import * # –§—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ fill_data.py

app = Flask(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
SAMPLE_RATE = 16000
AUDIO_FOLDER = "raw_audio"
SPECTROGRAM_FOLDER = "spectrograms"
BUFFER_SIZE = 24000  # 24000 —Å—ç–º–ø–ª–æ–≤ (= 1.5 —Å–µ–∫)
CHUNK_SIZE = 2048    # –ö–∞–∫ –≤ –ø—Ä–æ—à–∏–≤–∫–µ –ú–ö —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
EXPECTED_CHUNKS = (BUFFER_SIZE + CHUNK_SIZE - 1) // CHUNK_SIZE # 12 —á–∞–Ω–∫–æ–≤ (24000/2048)

os.makedirs(AUDIO_FOLDER, exist_ok=True)
os.makedirs(SPECTROGRAM_FOLDER, exist_ok=True)

# –¢—É—Ç –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏, —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
class ResNetSpectrogram(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.resnet = resnet18(pretrained=False)
        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=0, bias=False)
        self.resnet.avgpool = nn.Identity()
        self.resnet.fc = nn.Identity()
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def forward(self, x):
        x = self.resnet.conv1(x)
        x = self.resnet.bn1(x)
        x = self.resnet.relu(x)
        x = self.resnet.maxpool(x)
        
        x = self.resnet.layer1(x)
        x = self.resnet.layer2(x)
        x = self.resnet.layer3(x)
        x = self.resnet.layer4(x)
        
        x = self.adaptive_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å
def load_model(model_path, num_classes=3):
    model = ResNetSpectrogram(num_classes=num_classes)
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.eval()
    return model

# –¢—É—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def prepare_input(audio):
    # –î–µ–ª–∞—é float, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å –ú–ö –∏–¥–µ—Ç int16_t, –∞ –æ–±—É—á–∞–ª–∏ –Ω–∞ —Ñ–ª–æ–∞—Ç
    audio = audio.astype(np.float32)
    audio = librosa.util.normalize(audio)
    
    mel_spec = create_mel_spec(audio) # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
    input_data = normalize(mel_spec)
    
    # –î–æ–±–∞–≤–ª—è—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (1, 1, H, W)
    input_tensor = torch.FloatTensor(input_data).unsqueeze(0).unsqueeze(0)
    
    return input_tensor, mel_spec

# –°–æ—Ö—Ä–∞–Ω—è—é —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
def save_spectrogram_image(spec, filename):
    try:
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(spec, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel-frequency spectrogram')
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()  # –Ø–≤–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ —Ñ–∏–≥—É—Ä—ã
    except Exception as e:
        print(f"Error saving spectrogram: {e}")

# –ö–ª–∞—Å—Å—ã –∫–æ–º–∞–Ω–¥
CLASS_NAMES = ["–¥–∞—Ç–∞", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "–≤—Ä–µ–º—è", "—à—É–º"]

# –ó–∞–≥—Ä—É–∂–∞—é –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
model = load_model("best_model.pth", num_classes=len(CLASS_NAMES))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä –∑–¥–µ—Å—å
current_session = {
    "id": None,
    "chunks": [],
    "total_samples": 0
}

@app.route('/process_audio', methods=['POST'])
def process_audio():
    global current_session

    try:
        data = request.get_json()
        
        if 'audio' not in data:
            return jsonify({"error": "No audio data"}), 400
        
        chunk_index = data["chunk_index"]
        total_chunks = data["total_chunks"]
        audio_chunk = np.array(data["audio"], dtype=np.int16)
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ ‚Äî –Ω–∞—á–∏–Ω–∞—é –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
        if chunk_index == 0:
            current_session["id"] = int(time.time())
            current_session["chunks"] = []
            current_session["total_samples"] = 0
            print(f"\nüî• –ù–∞—á–∞—Ç–∞ –Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è {current_session['id']}")

        # –ü—Ä–æ–≤–µ—Ä—è—é, —á—Ç–æ —á–∞–Ω–∫ –ø—Ä–∏—à–µ–ª –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏
        if len(current_session["chunks"]) != chunk_index:
            print(f"‚õîÔ∏è –û—à–∏–±–∫–∞ –ø–æ—Ä—è–¥–∫–∞ —á–∞–Ω–∫–æ–≤! –û–∂–∏–¥–∞–ª—Å—è {len(current_session['chunks'])}, –ø–æ–ª—É—á–µ–Ω {chunk_index}")
            return jsonify({"error": "Invalid chunk order"}), 400

        # –î–æ–±–∞–≤–ª—è—é —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä
        current_session["chunks"].append(audio_chunk)
        current_session["total_samples"] += len(audio_chunk)
        
        print(f"‚úÖ –ß–∞–Ω–∫ {chunk_index + 1}/{total_chunks} –ø—Ä–∏–Ω—è—Ç ({len(audio_chunk)} —Å—ç–º–ø–ª–æ–≤)")

        # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—ë
        if chunk_index == total_chunks - 1:
            if current_session["total_samples"] >= BUFFER_SIZE - CHUNK_SIZE:  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                if current_session["total_samples"] != BUFFER_SIZE:
                    print(f"‚õîÔ∏è –û—à–∏–±–∫–∞! –ü–æ–ª—É—á–µ–Ω–æ {current_session['total_samples']}, –æ–∂–∏–¥–∞–ª–æ—Å—å {BUFFER_SIZE}")
                    return jsonify({"error": "Incomplete audio data"}), 400

            # –°–∫–ª–µ–∏–≤–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
            full_audio = np.concatenate(current_session["chunks"])
            
            # –°–æ—Ö—Ä–∞–Ω—è—é —Å—ã—Ä—ã–µ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ, –æ–ø—è—Ç—å –∂–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–æ—Ç–æ–º –ø—Ä–æ–≥—Ä–∞–º–º—ã
            audio_filename = f"{AUDIO_FOLDER}/audio_{current_session['id']}.pkl"
            with open(audio_filename, 'wb') as f:
                pickle.dump(full_audio, f)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
            input_tensor, mel_spec = prepare_input(full_audio)
            
            # –°–æ—Ö—Ä–∞–Ω—è—é –∫–∞—Ä—Ç–∏–Ω–∫—É —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∫–∏
            spec_image_filename = f"{SPECTROGRAM_FOLDER}/spec_{current_session['id']}.png"
            save_spectrogram_image(mel_spec, spec_image_filename)
            
            # –ì–∞–¥–∞–µ–º –≤ –≥–∞–¥–∞–ª–∫–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∫–∏
            with torch.no_grad():
                outputs = model(input_tensor)
                _, predicted = torch.max(outputs, 1)
                command = CLASS_NAMES[predicted.item()]
            
            print(f"\nüíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {audio_filename}")
            print(f"üìä –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {spec_image_filename}")
            print(f"ü§ñ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")
            
            return jsonify({
                "status": "success",
                "samples_received": full_audio.size,
                "command": command
            })
        else:
            return jsonify({"status": "chunk_received"})

    except Exception as e:
        print(f"‚õîÔ∏è –û—à–∏–±–∫–∞: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    print(f"üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ—Ç—Å—è {EXPECTED_CHUNKS} —á–∞–Ω–∫–æ–≤ –ø–æ {CHUNK_SIZE} —Å—ç–º–ø–ª–æ–≤")
    print(f"üîä –ö–ª–∞—Å—Å—ã –∫–æ–º–∞–Ω–¥: {CLASS_NAMES}")
    app.run(host='0.0.0.0', port=5000, debug=True)
